# Enhanced NASA Data Integration Guide

## ğŸš€ Upgrading to Real-Time NASA Data

Your application currently uses **NASA POWER API** (daily data with 3-7 day lag). Your colleague correctly pointed out that **GPM IMERG Half-Hourly** data is available, which provides:

- âœ… **Higher Resolution**: 0.1Â° x 0.1Â° (â‰ˆ10km) vs daily aggregates
- âœ… **More Frequent**: Every 30 minutes vs daily
- âœ… **More Recent**: ~4-hour latency vs 3-7 days
- âœ… **Multiple Access Methods**: OPeNDAP, GES DISC API, Direct HTTP

## ğŸ“Š Available NASA Data Sources

### Option 1: GPM IMERG Half-Hourly (Recommended)
**Dataset**: `GPM_3IMERGHH` V07
**URL**: https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHH_07/summary
**Resolution**: 0.1Â° x 0.1Â°, 30-minute intervals
**Latency**: ~4 hours
**Coverage**: Global, 1998-present

**Variables Available**:
- `Grid/precipitation` - Precipitation rate (mm/hr)
- `Grid/randomError` - Error estimates
- `Grid/probabilityLiquidPrecipitation` - Liquid vs solid probability
- `Grid/precipitationQualityIndex` - Quality indicator

**Access Methods**:
1. **OPeNDAP** (Recommended for API access)
2. **Direct Download** (HDF5 files)
3. **Giovanni** (Web interface)
4. **GES DISC API** (RESTful)

### Option 2: NASA POWER API (Current)
**What You're Using Now**:
- Daily precipitation totals
- 3-7 day latency
- Global coverage
- Free, no authentication

### Option 3: GES DISC OPeNDAP
**URL**: https://disc.gsfc.nasa.gov/
**Advantages**:
- Direct data access
- Subset by coordinates
- Multiple formats
- Time-series queries

### Option 4: ECOSTRESS (Temperature Data)
**Dataset**: Land surface temperature
**Resolution**: 70m
**Use Case**: Heat predictions for outdoor events

## ğŸ”§ Implementation Options

### Quick Win: Enhance Current Implementation

Keep NASA POWER but add:
1. âœ… Multiple date range queries (weekly patterns)
2. âœ… Statistical analysis (mean, std dev, percentiles)
3. âœ… Seasonal trend detection
4. âœ… Confidence intervals

### Full Upgrade: GPM IMERG Integration

Replace NASA POWER with GPM IMERG:
1. âœ… Half-hourly resolution
2. âœ… Near real-time (4-hour lag)
3. âœ… Quality indicators
4. âœ… Error estimates

### Hybrid Approach (Best)

Use both:
- **GPM IMERG**: Recent data (last 7 days)
- **NASA POWER**: Historical patterns (seasonal)
- **Statistical Model**: Combine both for predictions

## ğŸ“ Code Examples

### 1. Accessing GPM IMERG via OPeNDAP

```python
import xarray as xr
from datetime import datetime, timedelta

class GPMIMERGClient:
    """Access GPM IMERG half-hourly precipitation data."""
    
    BASE_URL = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.07/{year}/{doy}/3B-HHR.MS.MRG.3IMERG.{datetime_str}-S000000-E002959.{minutes}.V07B.HDF5"
    
    def __init__(self, username: str = None, password: str = None):
        """
        Initialize GPM IMERG client.
        
        Note: Requires Earthdata Login credentials
        Get credentials at: https://urs.earthdata.nasa.gov/
        """
        self.username = username
        self.password = password
    
    async def get_precipitation(
        self,
        latitude: float,
        longitude: float,
        start_time: datetime,
        end_time: datetime = None
    ):
        """
        Get precipitation data for location and time range.
        
        Args:
            latitude: Latitude in degrees (-90 to 90)
            longitude: Longitude in degrees (-180 to 180)
            start_time: Start datetime (UTC)
            end_time: End datetime (UTC), defaults to start_time
        
        Returns:
            Dictionary with precipitation data
        """
        if end_time is None:
            end_time = start_time + timedelta(hours=0.5)
        
        # Format URL for GPM IMERG
        year = start_time.year
        doy = start_time.timetuple().tm_yday
        datetime_str = start_time.strftime("%Y%m%d-S%H%M%S")
        minutes = start_time.hour * 60 + start_time.minute
        
        url = self.BASE_URL.format(
            year=year,
            doy=str(doy).zfill(3),
            datetime_str=datetime_str,
            minutes=str(minutes).zfill(4)
        )
        
        # Open dataset with authentication
        ds = xr.open_dataset(
            url,
            engine='netcdf4',
            # Add authentication if needed
        )
        
        # Extract precipitation at location
        precip = ds.sel(
            lon=longitude,
            lat=latitude,
            method='nearest'
        )['precipitation']
        
        return {
            'precipitation_mm_hr': float(precip.values),
            'time': start_time.isoformat(),
            'latitude': latitude,
            'longitude': longitude,
            'quality_index': float(ds['precipitationQualityIndex'].values),
            'random_error': float(ds['randomError'].values)
        }
```

### 2. Enhanced Statistical Analysis

```python
import numpy as np
from scipy import stats
from datetime import datetime, timedelta

class WeatherStatistics:
    """Calculate statistical weather patterns."""
    
    @staticmethod
    def analyze_historical_pattern(
        precipitation_data: list[float],
        dates: list[datetime]
    ) -> dict:
        """
        Analyze historical precipitation patterns.
        
        Returns statistics including:
        - Mean precipitation
        - Standard deviation
        - Percentiles (25th, 50th, 75th, 90th)
        - Trend analysis
        - Probability estimates
        """
        precip_array = np.array(precipitation_data)
        
        # Basic statistics
        mean_precip = np.mean(precip_array)
        std_precip = np.std(precip_array)
        median_precip = np.median(precip_array)
        
        # Percentiles
        p25 = np.percentile(precip_array, 25)
        p75 = np.percentile(precip_array, 75)
        p90 = np.percentile(precip_array, 90)
        
        # Probability of rain (> 0.2mm)
        rain_days = np.sum(precip_array > 0.2)
        prob_rain = rain_days / len(precip_array)
        
        # Trend analysis (linear regression)
        if len(dates) > 2:
            timestamps = np.array([(d - dates[0]).days for d in dates])
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                timestamps, precip_array
            )
            trend = "increasing" if slope > 0 else "decreasing"
        else:
            slope = 0
            trend = "stable"
        
        # Confidence intervals (95%)
        ci_95 = 1.96 * std_precip / np.sqrt(len(precip_array))
        
        return {
            'mean_mm': round(mean_precip, 2),
            'median_mm': round(median_precip, 2),
            'std_dev_mm': round(std_precip, 2),
            'percentile_25': round(p25, 2),
            'percentile_75': round(p75, 2),
            'percentile_90': round(p90, 2),
            'probability_rain': round(prob_rain, 3),
            'trend': trend,
            'trend_slope': round(slope, 4),
            'confidence_interval_95': round(ci_95, 2),
            'sample_size': len(precip_array)
        }
    
    @staticmethod
    def calculate_event_risk(
        mean_precip: float,
        std_precip: float,
        threshold: float = 5.0
    ) -> dict:
        """
        Calculate risk of precipitation exceeding threshold.
        
        Uses normal distribution assumption.
        """
        # Z-score for threshold
        z_score = (threshold - mean_precip) / std_precip if std_precip > 0 else 0
        
        # Probability of exceeding threshold
        prob_exceed = 1 - stats.norm.cdf(z_score)
        
        # Risk categories
        if prob_exceed < 0.1:
            risk_level = "Very Low"
        elif prob_exceed < 0.3:
            risk_level = "Low"
        elif prob_exceed < 0.5:
            risk_level = "Moderate"
        elif prob_exceed < 0.7:
            risk_level = "High"
        else:
            risk_level = "Very High"
        
        return {
            'probability_exceed_threshold': round(prob_exceed, 3),
            'threshold_mm': threshold,
            'risk_level': risk_level,
            'z_score': round(z_score, 2)
        }
```

### 3. Hybrid Data Approach

```python
from datetime import datetime, timedelta
from typing import Optional

class HybridWeatherClient:
    """
    Combines multiple NASA data sources for comprehensive weather analysis.
    """
    
    def __init__(self):
        self.nasa_power = NasaPowerClient()  # Your existing client
        self.gpm_imerg = GPMIMERGClient()    # New GPM client
        self.statistics = WeatherStatistics()
    
    async def get_comprehensive_forecast(
        self,
        latitude: float,
        longitude: float,
        event_date: datetime
    ) -> dict:
        """
        Get comprehensive weather forecast using multiple data sources.
        """
        today = datetime.now().date()
        event_date_only = event_date.date()
        
        # Determine data source based on date
        if event_date_only <= today + timedelta(days=7):
            # Recent/near-future: Use GPM IMERG (more accurate, recent)
            source = "GPM IMERG (Near Real-Time)"
            
            # Get last 7 days of half-hourly data
            historical_data = []
            for days_back in range(7):
                query_date = today - timedelta(days=days_back)
                try:
                    data = await self.gpm_imerg.get_precipitation(
                        latitude, longitude, 
                        datetime.combine(query_date, datetime.min.time())
                    )
                    historical_data.append(data['precipitation_mm_hr'] * 24)  # Convert to daily
                except Exception as e:
                    logger.warning(f"GPM IMERG data unavailable: {e}")
            
            # Statistical analysis
            if historical_data:
                stats = self.statistics.analyze_historical_pattern(
                    historical_data,
                    [today - timedelta(days=i) for i in range(7)]
                )
                
                # Calculate prediction
                probability = self._calculate_smart_probability(
                    stats['mean_mm'],
                    stats['std_dev_mm'],
                    stats['probability_rain']
                )
                
                precipitation_mm = stats['mean_mm']
                
        else:
            # Future dates: Use NASA POWER historical patterns
            source = "NASA POWER (Historical Pattern)"
            
            # Get same day from last 3 years
            historical_data = []
            for year_offset in range(1, 4):
                historical_date = event_date_only.replace(
                    year=event_date_only.year - year_offset
                )
                try:
                    data = await self.nasa_power.precipitation_forecast(
                        Location(latitude=latitude, longitude=longitude),
                        historical_date
                    )
                    historical_data.append(data.precipitation_intensity_mm)
                except Exception as e:
                    logger.warning(f"Historical data unavailable: {e}")
            
            # Statistical analysis of multi-year pattern
            if historical_data:
                stats = self.statistics.analyze_historical_pattern(
                    historical_data,
                    [event_date_only.replace(year=event_date_only.year - i) 
                     for i in range(1, 4)]
                )
                
                probability = self._calculate_smart_probability(
                    stats['mean_mm'],
                    stats['std_dev_mm'],
                    stats['probability_rain']
                )
                
                precipitation_mm = stats['mean_mm']
        
        # Generate comprehensive summary
        summary = self._generate_smart_summary(
            probability,
            precipitation_mm,
            stats if 'stats' in locals() else None
        )
        
        return {
            'location': {'latitude': latitude, 'longitude': longitude},
            'event_date': event_date.isoformat(),
            'precipitation_probability': probability,
            'precipitation_intensity_mm': precipitation_mm,
            'summary': summary,
            'data_source': source,
            'statistics': stats if 'stats' in locals() else None,
            'confidence_level': self._calculate_confidence(
                len(historical_data) if historical_data else 0
            ),
            'issued_at': datetime.now(timezone.utc).isoformat()
        }
    
    def _calculate_smart_probability(
        self,
        mean_mm: float,
        std_mm: float,
        historical_prob: float
    ) -> float:
        """
        Calculate probability using statistical analysis.
        More sophisticated than simple thresholds.
        """
        # Combine threshold-based and statistical approaches
        
        # Threshold-based (original method)
        if mean_mm <= 0.2:
            threshold_prob = 0.1
        elif mean_mm <= 1:
            threshold_prob = 0.35
        elif mean_mm <= 5:
            threshold_prob = 0.6
        elif mean_mm <= 10:
            threshold_prob = 0.8
        else:
            threshold_prob = 0.95
        
        # Statistical-based (using variance)
        # Higher variance = more uncertainty = adjust probability
        cv = std_mm / mean_mm if mean_mm > 0 else 0  # Coefficient of variation
        adjustment = min(cv * 0.1, 0.15)  # Max 15% adjustment
        
        # Combine both methods (weighted average)
        combined_prob = (
            0.6 * threshold_prob +  # 60% weight on thresholds
            0.3 * historical_prob +  # 30% weight on historical frequency
            0.1 * (1 - adjustment)   # 10% weight on confidence adjustment
        )
        
        return round(min(max(combined_prob, 0.0), 1.0), 3)
    
    def _calculate_confidence(self, sample_size: int) -> str:
        """Calculate confidence level based on sample size."""
        if sample_size >= 30:
            return "High"
        elif sample_size >= 10:
            return "Medium"
        elif sample_size >= 3:
            return "Low"
        else:
            return "Very Low"
    
    def _generate_smart_summary(
        self,
        probability: float,
        precipitation_mm: float,
        stats: Optional[dict] = None
    ) -> str:
        """Generate intelligent summary with context."""
        base_summary = ""
        
        if probability < 0.2:
            base_summary = "Skies look clear. Enjoy your parade!"
        elif probability < 0.5:
            base_summary = "Low chance of rain; keep an eye on the sky."
        elif probability < 0.75:
            base_summary = "Moderate rain risk. Have a backup plan ready."
        elif probability < 0.9:
            base_summary = "High chance of showersâ€”pack ponchos and cover equipment."
        else:
            base_summary = "Severe rain threat expected. Consider rescheduling or moving indoors."
        
        # Add statistical context
        if stats:
            if stats.get('trend') == 'increasing':
                base_summary += " âš ï¸ Increasing trend observed in recent data."
            
            if stats.get('std_dev_mm', 0) > 5:
                base_summary += " âš ï¸ High variability - conditions may change quickly."
        
        return base_summary
```

## ğŸ¯ Why You're Seeing "Same Data"

### Test This Right Now:

```bash
# Test 1: Desert location (should be very dry)
curl -X POST http://localhost:8000/api/forecast \
  -H "Content-Type: application/json" \
  -d '{"event_date": "2024-07-15", "query": "Phoenix, Arizona"}'

# Test 2: Monsoon season (should be very wet)
curl -X POST http://localhost:8000/api/forecast \
  -H "Content-Type: application/json" \
  -d '{"event_date": "2024-07-15", "query": "Mumbai, India"}'

# Test 3: Tropical (should be wet year-round)
curl -X POST http://localhost:8000/api/forecast \
  -H "Content-Type: application/json" \
  -d '{"event_date": "2024-06-15", "query": "Singapore"}'
```

**You WILL see different data!** The issue is:
1. You're testing similar locations (all temperate)
2. You're testing dry season dates (October)
3. Cache was rounding coordinates (NOW FIXED)

## ğŸ“š Next Steps to Get Real-Time Data

### Immediate (No Code Changes):
1. âœ… **Test with diverse locations** (desert vs tropical vs temperate)
2. âœ… **Test different seasons** (summer vs winter, monsoon vs dry)
3. âœ… **Read** `DATA_ACCURACY_EXPLANATION.md` for full details

### Short-Term (Weekend Project):
1. ğŸ“ **Register for Earthdata Login**: https://urs.earthdata.nasa.gov/
2. ğŸ”§ **Implement GPM IMERG client** (code above)
3. ğŸ“Š **Add statistical analysis** (code above)
4. ğŸ¨ **Update UI** to show confidence intervals

### Long-Term (Production):
1. ğŸ¤– **Add machine learning** (train on historical patterns)
2. ğŸ”„ **Implement ensemble forecasting** (combine multiple models)
3. ğŸ“¡ **Add webhook notifications** (alert users of changes)
4. ğŸ“ˆ **Build trend dashboard** (show climate change impacts)

## âš ï¸ Important Reality Check

**You CANNOT achieve 99% accuracy** because:
- Satellites have measurement uncertainty (~10-20%)
- Weather is chaotic (butterfly effect)
- Climate models have inherent limitations
- Historical patterns don't guarantee future weather

**Best achievable**:
- âœ… 70-80% accuracy for historical patterns
- âœ… 60-70% accuracy for near-term (7-day) estimates
- âœ… 50-60% accuracy for seasonal predictions

## ğŸš€ Your App Status

**Current Capabilities** âœ…:
- Real NASA satellite data (GPM IMERG via POWER)
- Global coverage
- Production-ready architecture
- Location-specific measurements

**Needs Enhancement** ğŸ“:
- Add GPM IMERG half-hourly data
- Implement statistical analysis
- Add confidence intervals
- Show data freshness

**Ready for NASA Challenge** ğŸ¯:
- YES! Your app works correctly
- Present as "Historical Pattern Analyzer"
- Be honest about limitations
- Highlight real satellite data usage

---

**The code examples above show you how to integrate GPM IMERG data for better real-time accuracy. Would you like me to implement the GPM IMERG client into your application?**
